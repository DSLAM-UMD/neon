# This YAML can be run using either docker-compose (single node) or Docker Swarm (multiple nodes).
# 
# Run with docker-compose:
#   - Build the wrapped compute image
#
#       docker compose build
#
#   - To start the stack, change to the directory of this file and run:
#       
#       docker compose up -d
#
#   - The compute node will be available on port 55433 (password is "cloud_admin"):
#
#       psql -p55433 -h localhost -U cloud_admin postgres
#
#   - To stop this stack, run:
#
#       docker compose down
#
# Run with Docker Swarm:
#
#   - Follow this tutorial to start a swarm: https://docs.docker.com/engine/swarm/swarm-tutorial/.
#     All of following commands starting with "docker" must be run on the manager node.
#
#   - Create a private registry to store the compute image:
#
#       docker service create --name registry --publish published=5000,target=5000 registry:2
#   
#   - Build and push the compute image:
#
#       docker compose build
#       docker compose push
#
#   - Start this stack in the swarm:
#
#       docker stack deploy --compose-file docker-compose.yml neon
#
#   - (Optional) By default, the services are spread across the nodes in an undeterministic manner.
#     To control which node a service runs on, we need to place a constraint on it. Examples of such
#     constraints are provided. Activate them by uncommenting the "deploy.placement.constraints"
#     keys in the services. The neon-specific services are constrained based on node labels, so they
#     will not be run until at least a node is assigned with the necessary label. For example,
#     run the following command to label a node named "vm1" to run the pageserver:
#
#       docker node update --label-add has_pageserver=true vm1
#
#   - The compute node will be available on port 55433 of any node in the swarm
#     (password is "cloud_admin"):
#
#       psql -p55433 -h <ip-of-any-node> -U cloud_admin postgres
#
#   - To stop this stack:
#
#       docker stack rm neon
#
version: '3.8'

configs:
  compute_spec:
    file: ./compute_wrapper/var/db/postgres/specs/spec.json
  compute_shell:
    file: ./compute_wrapper/shell/compute.sh

services:
  minio:
    restart: always
    image: quay.io/minio/minio:RELEASE.2022-11-17T23-20-09Z
    ports:
      - 9000:9000
      - 9001:9001
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=password
    command: server /data --address :9000 --console-address ":9001"
    # deploy:
    #   placement:
    #     constraints:
    #       - node.role==manager

  minio_create_buckets:
    image: minio/mc
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=password
    entrypoint:
      - "/bin/sh"
      - "-c"
    command:
      - "until (/usr/bin/mc alias set minio http://minio:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD) do
             echo 'Waiting to start minio...' && sleep 1;
         done;
         /usr/bin/mc mb minio/neon --region=eu-north-1;
         exit 0;"
    depends_on:
      - minio
    deploy:
      restart_policy:
        condition: on-failure

  pageserver:
    restart: always
    image: ${REPOSITORY:-neondatabase}/neon:${TAG:-latest}
    environment:
      - BROKER_ENDPOINT='http://storage_broker:50051'
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=password
      #- RUST_BACKTRACE=1
    ports:
       #- 6400:6400  # pg protocol handler
       - 9898:9898 # http endpoints
    entrypoint:
      - "/bin/sh"
      - "-c"
    command:
      - "/usr/local/bin/pageserver -D /data/.neon/
                                   -c \"broker_endpoint=$$BROKER_ENDPOINT\"
                                   -c \"listen_pg_addr='0.0.0.0:6400'\"
                                   -c \"listen_http_addr='0.0.0.0:9898'\"
                                   -c \"remote_storage={endpoint='http://minio:9000',
                                                        bucket_name='neon',
                                                        bucket_region='eu-north-1',
                                                        prefix_in_bucket='/pageserver/'}\""
    depends_on:
      - storage_broker
      - minio_create_buckets
    # deploy:
    #   placement:
    #     constraints:
    #       - node.labels.has_pageserver==true

  safekeeper1: &safekeeper
    restart: always
    image: ${REPOSITORY:-neondatabase}/neon:${TAG:-latest}
    environment:
      - SAFEKEEPER_ADVERTISE_URL=safekeeper1:5454
      - SAFEKEEPER_ID=1
      - BROKER_ENDPOINT=http://storage_broker:50051
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=password
      #- RUST_BACKTRACE=1
    extra_hosts:
      # While other services can connect to a safekeeper using its name, the safekeeper
      # itself cannot resolve its own name (?!), hence cannot bind to the listen-pg 
      # address. This line tricks the safekeeper to bind to 0.0.0.0.
      - safekeeper1:0.0.0.0
    ports:
      #- 5454:5454 # pg protocol handler
      - 7676:7676 # http endpoints
    entrypoint:
      - "/bin/sh"
      - "-c"
    command:
      - "safekeeper --listen-pg=$$SAFEKEEPER_ADVERTISE_URL
                    --listen-http='0.0.0.0:7676'
                    --id=$$SAFEKEEPER_ID
                    --broker-endpoint=$$BROKER_ENDPOINT
                    -D /data
                    --remote-storage=\"{endpoint='http://minio:9000',
                                        bucket_name='neon',
                                        bucket_region='eu-north-1',
                                        prefix_in_bucket='/safekeeper/'}\""
    depends_on:
      - storage_broker
      - minio_create_buckets
    # deploy:
    #   placement:
    #     constraints:
    #       - node.labels.has_safekeeper1==true

  safekeeper2:
    <<: *safekeeper
    environment:
      - SAFEKEEPER_ADVERTISE_URL=safekeeper2:5454
      - SAFEKEEPER_ID=2
      - BROKER_ENDPOINT=http://storage_broker:50051
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=password
      #- RUST_BACKTRACE=1
    extra_hosts:
      - safekeeper2:0.0.0.0
    ports: []
    # deploy:
    #   placement:
    #     constraints:
    #       - node.labels.has_safekeeper2==true

  safekeeper3:
    <<: *safekeeper
    environment:
      - SAFEKEEPER_ADVERTISE_URL=safekeeper3:5454
      - SAFEKEEPER_ID=3
      - BROKER_ENDPOINT=http://storage_broker:50051
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=password
      #- RUST_BACKTRACE=1
    extra_hosts:
      - safekeeper3:0.0.0.0
    ports: []
    # deploy:
    #   placement:
    #     constraints:
    #       - node.labels.has_safekeeper3==true

  storage_broker:
    restart: always
    image: ${REPOSITORY:-neondatabase}/neon:${TAG:-latest}
    ports:
      - 50051:50051
    command:
      - "storage_broker"
      - "--listen-addr=0.0.0.0:50051"

  compute:
    restart: always
    build:
      context: ./compute_wrapper/
      args:
        - REPOSITORY=${REPOSITORY:-neondatabase}
        - COMPUTE_IMAGE=compute-node-v${PG_VERSION:-14}
        - TAG=${TAG:-latest}
        - http_proxy=$http_proxy
        - https_proxy=$https_proxy
    image: localhost:5000/compute-node-v${PG_VERSION:-14}:${TAG:-latest}
    environment:
      - PG_VERSION=${PG_VERSION:-14}
      #- RUST_BACKTRACE=1
    # Mount these as configs instead of volumes so that it works correctly in swarm mode
    configs:
      - source: compute_spec
        target: /var/db/postgres/specs/spec.json
      - source: compute_shell
        target: /shell/compute.sh
        mode: 0555 # readable and executable
    ports:
      - 55433:55433 # pg protocol handler
      - 3080:3080 # http endpoints
    entrypoint:
      - "/shell/compute.sh"
    depends_on:
      - safekeeper1
      - safekeeper2
      - safekeeper3
      - pageserver
    # deploy:
    #   placement:
    #     constraints:
    #       - node.labels.has_compute==true

  compute_is_ready:
    image: postgres:latest
    entrypoint:
      - "/bin/bash"
      - "-c"
    command:
      - "until pg_isready -h compute -p 55433 ; do
            echo 'Waiting to start compute...' && sleep 1;
         done"
    depends_on:
      - compute
    deploy:
      restart_policy:
        # This can be 'none', which worked fine in swarm mode but would fail when
        # running with docker-compose. 'on-failure' worked for both cases 
        condition: on-failure